%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Masterthesis in DataScience
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sean Kallenberg, University of Trier, 2020
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,12pt,titlepage,enabledeprecatedfontcommands]{scrreprt} 

\usepackage[T1]{fontenc}            
\usepackage[utf8]{inputenc}         
\usepackage{amsmath,amsfonts, amssymb, amsthm, mathrsfs}
\usepackage{bbm}                    
\usepackage{graphicx}               
\usepackage{textcomp,doc}
\usepackage{cmbright}               
\usepackage{courier}                
\usepackage{color}							
\usepackage{exscale}						
\usepackage[english]{babel}		
\usepackage{times}						
\usepackage{graphicx}						
\usepackage{pgf}	
\usepackage{listings}
\usepackage{cite}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{bm}
\usepackage{url}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{subfig}


\parskip1ex                         
\parindent0mm                       

\newtheorem{definition}{Definition}[chapter]
\newtheorem{example}[definition]{Beispiel}
\newtheorem{theorem}[definition]{Theorem}

\begin{document}


\begin{titlepage}
\phantom{erste Zeile}
\begin{center}
\Huge
\textbf{Multigrid Methods \\ in \\Neuronal Networks}\\
\vspace{0.5cm}
\large
\vspace{3cm}
MASTER THESIS \\
\vspace{0.5cm}
to obtain the academic degree of \\
Master of Science \\
\vspace{1cm}
submitted to the Department IV - Data Science\\
of the University of Trier \\
\vspace{2.5cm}
by \\
\vspace{1cm}
Sean, Kallenberg,\\
Zurmaienerstraße 166 \\
54292 Trier \\
\vspace{1cm}
Supervisor: Prof. Dr. Volker Schulz \\
\vspace{1cm}
Trier, \today         
\end{center}
\normalsize
\vfill
\end{titlepage}

\pagenumbering{roman}
\large
\vspace{2.5cm}
\begin{center}
\textbf{Statutory Declaration}\\
\end{center}
\vspace{0.5cm}
\normalsize
I hereby declare that I wrote this thesis independently and have not used any other source or tool than the ones indicated. Any thought taken directly or indirectly from external sources are marked as such. This master thesis was not presented to any other examination office in the same or similar form. It was not yet published.

\vspace{9cm}

\today\\             

\smallskip
\small\hspace{0cm}(Date) \hspace{8cm} (Signature)
\normalsize
\newpage

\tableofcontents
\listoffigures

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\chapter{Introduction}
The task of image analysis has a steadily growing importance in everyday situations. Whether it is in form of classification, segmentation or object detection - many parts of modern applications rely on the technology and the results reachable today. This is mostly thanks due to the astounding precision the research community was able to produce using artificial neuronal networks (ANN). \\
An ANN aims to approximate complex or simply unknown relations of incoming data to a target space. For this task it uses blocks of linear transformations and links them together through non-linear functions, so-called activation functions. The combination of a linear block and non-linear activation is then called a neuron. A network consists of many layers consisting of differing amounts of neurons. In general each neuron in a layer will propagate its output to  all neurons in the next layer. The result of this chain is then fed into a transformation that maps onto the target space. Networks that follow this structure are accordingly named feed-forward networks, and the process the input undergoes is called forward propagation. Before producing sensible results the network has to be repeatedly trained on a correctly labeled data set where in each iteration a loss metric is computed to correct the weights in the linear transformation until they are able to correctly reproduce the data-target relationship. This is then correspondingly called backpropagation. \\
The term neuronal network stems from the loose resemblance of the structure of a feed-forward network to a neuronal system as each artificial neuron signals many other neurons, and if their connection produces sensible results in terms of the loss metric the weights inside the neuron should adapt to strengthen this connection. \\
The idea to base computational models on the neuronal system of the human brain was first introduced in the 1940s by Warren McCulloch and Warren Pitts \cite{10.5555/65669.104377} and quickly further build upon by Donald Hebb \cite{hebb-organization-of-behavior-1949}. But soon research reached its first bottleneck by the limited computational resources at the time as stated by Marvin Minsky and Seymour Papert \cite{minsky69perceptrons}. It was not possible to build and train large networks that would produce results that would justify the computational effort. Thus the idea of a learned model to classify input was replaced through systems based on if-then decisions. Nonetheless Ivakhnenko and Lapa published the first functional multi-layer networks in the mid 60s \cite{ivakhnenko1967cybernetics}.\\
Paul John Werbos introduced the backprogagation algorithm, which is used to this day, in 1975 \cite{werbos1975beyond}. Even though it allowed for efficient training of large neuronal networks, in most use cases they were still overtook by classifiers with a mathematical exact solution like support vector machines or linear regressions. In this time the fields neural networks were able to dominate in were medical applications such as predicting in the structure of proteins in the 1980s \cite{Rost1993PredictionOP}.
It took until the new millennium and the invention of high performing GPUs to make the training of very large networks feasible and thus superhuman precision possible. With the development of stronger GPUs the interest in ANNs steadily rose. New types of networks appeared, like convolutional and recurrent neuronal networks and with them new problems to overcome. The research group around Jürgen Schmidhuber was able to steadily improve the architecture of neuronal networks for pattern recognition and machine learning, e.g. in the task of handwriting recognition \cite{NIPS2008_3449}. \\
Dan Ciresan and colleagues, including aforementioned Schmidhuber, were the first to achieve human-competitive performance on benchmark tasks like traffic sign recognition \cite{2012arXiv1202.2745C}. In 2011 the fist convolutional neuronal networks (short CNN) were published \cite{CiresanConv} and achieved superhuman precision in image classification \cite{ciresan2012multicolumn}. \\
Since then there were many architectural breakthroughs like residual and dense networks that further upped the limit to what is possible. But one connection applies to all kinds of network: the more layers and therefore more parameters a network has the higher is the possible precision achievable. This simple comes from the increasing non-linearity which allows for a better approximation and can be also be observed in other predictive models. But with the number of parameters already being extremely high, reaching into the tens of millions, it quickly becomes difficult to train those models in a timely manner. This creates the need to research network architectures that are able to maintain high precision but reduce the computational overhead. \\
In this work we want to study one of the possible solutions to this problem by utilizing a well known and thoroughly studied method used in mathematical optimization: Multigrid algorithms. These originally aim to solve equation systems in a efficient manner by approximating the solution of the underlying problem on a coarser grid and then use this coarse result to improve the initial guess for the search of the wanted solution. This has been transferred to the architecture of CNNs by performing multigrid cycles through the network layers.
By using shared operators for grids of the same size we reduce the number of to be trained parameters by a large amount. We want to study the possibility of multigrid methods incorporated in CNNs to reduce computational overhead while keeping the precision at a competitive level in comparison to their regular counterparts.
This work is structured as such: First we are going to introduce the standalone concept of multigrid methods in a mathematical context. There the core idea behind those methods as well as the most common algorithms are going to be explored and the convergence theory behind them elaborated. \\
Secondly we need to introduce a precise definition of ANNs and CNNs to be able to integrate multigrid methods. There we will also introduce sophisticated architectures for CNNs which we will later use to examine the results of the algorithms. \\
Afterwards the actual combination of multigrid and CNN is defined and illustrated before we lastly explain our implementation of the networks and analyze the results of the chosen architectures in a direct comparison. As conclusion we aim to answer the following question:\\
Is the incorporation of multigrid methods in a convolutional neuronal network able to either increase performance or to maintain performance and reduce computational overhead on the benchmark task of image classification?

\chapter{Related Work}
Convolutional neuronal networks have been the subject to a wide field of previous works. Ian Goodfellow et al published a comprehensive guide to the fundamentals of deep learning and CNNs \cite{Goodfellow-et-al-2016}, to which we will refer to when covering the theory of ANNs. \\
Since the introduction of CNNs there where many papers that proposed architectures with groundbreaking results. The most commonly cited are LeNet-5 (1998) \cite{LeNet}, AlexNet (2012) \cite{AlexNet}, ZFNet (2013) \cite{zeiler2013visualizing}, GoogleNet (2014)\cite{szegedy2014going} and VGG-16 (2015) \cite{simonyan2015deep}. Each surpassed the previous one and improved the foundation for future works.\\ Two notable architectural innovations of the recent years are ResNet and DenseNet, which can be seen as successor to AlexNet and VGG-16. Many improvements proposed today are based on those networks in connection with new found regularization or data augmentation techniques, like EfficientNet. Other networks follow the path of GoogleNet, like PolyNet and Inception-v4, which is a direct improvement of GoogleNet by the same research team. These architectures work with parallel but segregated paths in each transformation block that are concatenated between layers in contrast to ResNet that operates with a more classic feed-forward approach. DenseNet can be seen as middle-ground between Inception-like networks and ResNet-like networks as it utilizes parallel paths in each block but they share the same applied operations. A visualization of these types can be found in FigureXYZ. \\

Parameter Reduction
Stochastic Depth
FractalNet DropPath
Dropout

Multigrid
Hackbusch
Briggs
Brandt
Ascher und Petzold

Multigrid in CNN
MGNet
LM ResNet
Multigrid in NeuralNetworks
UNet

\chapter{Multigrid Methods}
\section{Fundamentals}
\section{Iterative Algorithms}
\section{Convergence Theory}
\chapter{Artificial Neuronal Networks}
\section{Concept}
\section{Convolutional Networks}
\section{Sophisticated Network Architectures}
\chapter{Application of Multigrid Methods in CNNs}
\chapter{Implementation and empirical Analysis}
\section{Chosen Network Architectures}
\section{Realization in Python}
\section{Analysis}
\chapter{Conclusion}

\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{references} \label{bibtex}
\end{document}