%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Masterthesis in DataScience
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sean Kallenberg, University of Trier, 2020
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,12pt,titlepage,enabledeprecatedfontcommands]{scrreprt} 

\usepackage[T1]{fontenc}            
\usepackage[utf8]{inputenc}         
\usepackage{amsmath,amsfonts, amssymb, amsthm, mathrsfs}
\usepackage{bbm}                    
\usepackage{graphicx}               
\usepackage{textcomp,doc}
\usepackage{cmbright}               
\usepackage{courier}                
\usepackage{color}							
\usepackage{exscale}						
\usepackage[english]{babel}		
\usepackage{times}						
\usepackage{graphicx}						
\usepackage{pgf}	
\usepackage{listings}
\usepackage{cite}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{bm}
\usepackage{url}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{subfig}


\parskip1ex                         
\parindent0mm                       

\newtheorem{definition}{Definition}[chapter]
\newtheorem{example}[definition]{Beispiel}
\newtheorem{theorem}[definition]{Theorem}

\begin{document}


\begin{titlepage}
\phantom{erste Zeile}
\begin{center}
\Huge
\textbf{Multigrid Methods \\ in \\Neuronal Networks}\\
\vspace{0.5cm}
\large
\vspace{3cm}
MASTER THESIS \\
\vspace{0.5cm}
to obtain the academic degree of \\
Master of Science \\
\vspace{1cm}
submitted to the Department IV - Data Science\\
of the University of Trier \\
\vspace{2.5cm}
by \\
\vspace{1cm}
Sean, Kallenberg,\\
Zurmaienerstraße 166 \\
54292 Trier \\
\vspace{1cm}
Supervisor: Prof. Dr. Volker Schulz \\
\vspace{1cm}
Trier, \today         
\end{center}
\normalsize
\vfill
\end{titlepage}

\pagenumbering{roman}
\large
\vspace{2.5cm}
\begin{center}
\textbf{Statutory Declaration}\\
\end{center}
\vspace{0.5cm}
\normalsize
I hereby declare that I wrote this thesis independently and have not used any other source or tool than the ones indicated. Any thought taken directly or indirectly from external sources are marked as such. This master thesis was not presented to any other examination office in the same or similar form. It was not yet published.

\vspace{9cm}

\today\\             

\smallskip
\small\hspace{0cm}(Date) \hspace{8cm} (Signature)
\normalsize
\newpage

\tableofcontents
\listoffigures

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\chapter{Introduction}
The task of image analysis has a steadily growing importance in everyday situations. Whether it is in form of classification, segmentation or object detection - many parts of modern applications rely on the technology and the results reachable today. This is mostly thanks due to the astounding precision the research community was able to produce using artificial neuronal networks (ANN). \\
An ANN aims to approximate complex or simply unknown relations of incoming data to a target space. For this task it uses blocks of linear transformations and links them together through non-linear functions, so-called activation functions. The combination of a linear block and non-linear activation is then called a neuron. A network consists of many layers consisting of differing amounts of neurons. In general each neuron in a layer will propagate its output to  all neurons in the next layer. The result of this chain is then fed into a transformation that maps onto the target space. Networks that follow this structure are accordingly named feed-forward networks, and the process the input undergoes is called forward propagation. Before producing sensible results the network has to be repeatedly trained on a correctly labeled data set where in each iteration a loss metric is computed to correct the weights in the linear transformation until they are able to correctly reproduce the data-target relationship. This is then correspondingly called backpropagation. \\
The term neuronal network stems from the loose resemblance of the structure of a feed-forward network to a neuronal system as each artificial neuron signals many other neurons, and if their connection produces sensible results in terms of the loss metric the weights inside the neuron should adapt to strengthen this connection. \\
The idea to base computational models on the neuronal system of the human brain was first introduced in the 1940s by Warren McCulloch and Warren Pitts \cite{10.5555/65669.104377} and quickly further build upon by Donald Hebb \cite{hebb-organization-of-behavior-1949}. But soon research reached its first bottleneck by the limited computational resources at the time as stated by Marvin Minsky and Seymour Papert \cite{minsky69perceptrons}. It was not possible to build and train large networks that would produce results that would justify the computational effort. Thus the idea of a learned model to classify input was replaced through systems based on if-then decisions. Nonetheless Ivakhnenko and Lapa published the first functional multi-layer networks in the mid 60s \cite{ivakhnenko1967cybernetics}.\\
Paul John Werbos introduced the backprogagation algorithm, which is used to this day, in 1975 \cite{werbos1975beyond}. Even though it allowed for efficient training of large neuronal networks, in most use cases they were still overtook by classifiers with a mathematical exact solution like support vector machines or linear regressions. In this time the fields neural networks were able to dominate in were medical applications such as predicting in the structure of proteins in the 1980s \cite{Rost1993PredictionOP}.
It took until the new millennium and the invention of high performing GPUs to make the training of very large networks feasible and thus superhuman precision possible. With the development of stronger GPUs the interest in ANNs steadily rose. New types of networks appeared, like convolutional and recurrent neuronal networks and with them new problems to overcome. The research group around Jürgen Schmidhuber was able to steadily improve the architecture of neuronal networks for pattern recognition and machine learning, e.g. in the task of handwriting recognition \cite{NIPS2008_3449}. \\
Dan Ciresan and colleagues, including aforementioned Schmidhuber, were the first to achieve human-competitive performance on benchmark tasks like traffic sign recognition \cite{2012arXiv1202.2745C}. In 2011 the fist convolutional neuronal networks (short CNN) were published \cite{CiresanConv} and achieved superhuman precision in image classification \cite{ciresan2012multicolumn}. \\
Since then there were many architectural breakthroughs like residual and dense networks that further upped the limit to what is possible. But one connection applies to all kinds of network: the more layers and therefore more parameters a network has the higher is the possible precision achievable. This simple comes from the increasing non-linearity which allows for a better approximation and can be also be observed in other predictive models. But with the number of parameters already being extremely high, reaching into the tens of millions, it quickly becomes difficult to train those models in a timely manner. This creates the need to research network architectures that are able to maintain high precision but reduce the computational overhead. \\
In this work we want to study one of the possible solutions to this problem by utilizing a well known and thoroughly studied method used in mathematical optimization: Multigrid algorithms. These originally aim to solve equation systems in a efficient manner by approximating the solution of the underlying problem on a coarser grid and then use this coarse result to improve the initial guess for the search of the wanted solution. This has been transferred to the architecture of CNNs by performing multigrid cycles through the network layers.
By using shared operators for grids of the same size we reduce the number of to be trained parameters by a large amount. We want to study the possibility of multigrid methods incorporated in CNNs to reduce computational overhead while keeping the precision at a competitive level in comparison to their regular counterparts.
This work is structured as such: First we are going to introduce the standalone concept of multigrid methods in a mathematical context. There the core idea behind those methods as well as the most common algorithms are going to be explored and the convergence theory behind them elaborated. \\
Secondly we need to introduce a precise definition of ANNs and CNNs to be able to integrate multigrid methods. There we will also introduce sophisticated architectures for CNNs which we will later use to examine the results of the algorithms. \\
Afterwards the actual combination of multigrid and CNN is defined and illustrated before we lastly explain our implementation of the networks and analyze the results of the chosen architectures in a direct comparison. As conclusion we aim to answer the following question:\\
Is the incorporation of multigrid methods in a convolutional neuronal network able to either increase performance or at least reduce computational overhead on the benchmark task of image classification?

\chapter{Related Work}
Convolutional neuronal networks have been the subject to a wide field of previous works. Ian Goodfellow et al published a comprehensive guide to the fundamentals of deep learning and CNNs \cite{Goodfellow-et-al-2016}, to which we will refer to when covering the theory of ANNs. \\
Since the introduction of CNNs there where many papers that proposed architectures with groundbreaking results. The most commonly cited are LeNet-5 (1998) \cite{LeNet}, AlexNet (2012) \cite{AlexNet}, ZFNet (2013) \cite{zeiler2013visualizing}, GoogleNet (2014)\cite{szegedy2014going} and VGG-16 (2015) \cite{simonyan2015deep}. Each surpassed the previous one and improved the foundation for future works.\\ Two notable architectural innovations of the recent years are ResNet \cite{DBLP:journals/corr/HeZRS15} and DenseNet \cite{DBLP:journals/corr/HuangLW16a}, which can be seen as successor to AlexNet and VGG-16. Many improvements proposed today are based on those networks in connection with new found regularization or data augmentation techniques, like EfficientNet \cite{DBLP:journals/corr/abs-1905-11946}. Other networks follow the path of GoogleNet, like PolyNet \cite{DBLP:journals/corr/ZhangLLL16} and Inception-v4 \cite{DBLP:journals/corr/SzegedyIV16}, which is a direct improvement of GoogleNet by the same research team. These architectures work with parallel but segregated paths in each transformation block that are concatenated between layers in contrast to ResNet that operates with a more classic feed-forward approach. DenseNet can be seen as middle-ground between Inception-like networks and ResNet-like networks as it utilizes parallel paths in each block but they share the same applied operations. A visualization of these types can be found in FigureXYZ. \\
%% INSERT FIGURE OF BLOCK: VGG, GOOGLENET, RESNET, DENSENET
Closely related to parameter reduction is the handling of overfitting and regularization. The first method that tackles this problem that should be listed is the Dropout-layer ,which works through deactivating neurons in the forward propagation with a certain probability and thus randomizing which weights are applied to the input \cite{JMLR:v15:srivastava14a}. \\
Secondly we want to name the stochastic depth approach build directly upon the ResNet \cite{DBLP:journals/corr/HuangSLSW16}. Each residual block of the network has a given probability to be replaced by a identity projection to the next block which can drastically reduce the number of layers in the model. The replacement probability can be set constant or proportional to the position of the block. This randomizes the depth of the network in each iteration.
A similar method is applied in the FractalNet \cite{DBLP:journals/corr/LarssonMS16a}. Each block of the FractalNet consists of multiple parallel paths which are partially concatenated throughout the block. The DropPath method works by either randomly deactivating single operations in each block or deactivating all but one path in a block. This creates a high degree of randomization and simultaneously reduces the amount of parameters by a large factor. \\
As all of these approaches deactivate parts of the network they can also be seen as method for parameter reduction.
%% INSERT VISUALIZATION

Contrary to CNNs the theory around multigrid methods is thoroughly studied and concluded.
Multigrid methods where first proposed in the early 1960s by Radii Petrovich Fedorenko to solve the Poisson equation in a unit square \cite{FEDORENKO19621092,FEDORENKO1964227} and were quickly build upon by Nikolai Sergeevich Bakhvalov, who extended the applications to more general elliptic boundary value problems \cite{BAKHVALOV1966101}. Wolfang Hackbusch's released work in the late 70s further studied the efficiency of multigrid methods and introduced important convergence results for the field \cite{Hackbusch1977}. He first proposed the full multigrid scheme (FMG). Next to Hackbusch Achi Brandt proposed the Full Approximation Scheme (FAS) for multigrid methods which extends their applications to non-linear problems \cite{Brandt1973,Brandt1977}. In the late 1990s algebraic multigrid methods were introduced which apply the multigrid techniques to problems unrelated to differential equations \cite{stueben1999}. This approach was further generalized in the recent years and made applicable to abstract problems \cite{xu_zikatanov_2017}. \\

The combination of multigrid methods and neuronal networks was only explored in recent years and is open to further studying.\\
In 2015 the UNet was proposed for the task of image segmentation by Ronneberger et al. \cite{DBLP:journals/corr/RonnebergerFB15}. The UNet architecture can be compared to a multigrid V-cycle as it scales the input down to a low resolution and then back up to its original form. Additionally it adds the result of the downscaling process to the input of the same size in the upscaling through concatenation. \\
Regarding the task of image classification there where two separate approaches to incorporate multigrid methods. The first was proposed by Ke et al. in 2016 and implements the idea of working on different grid sizes by building three parallel network paths, each starting with a different input resolution. Before a convolutional layer is applied, the outputs of the paths are up or downscaled to the resolution of the adjacent path and then both concatenated to form the input for the convolution. Even though this architecture can produce competitive resutls to regular networks, it created massive computational overhead. \\
The approach which will be the focus of this work was first proposed in 2017 be Lu et al. \cite{DBLP:journals/corr/abs-1710-10121} and further generalized by He at al. in 2019 \cite{DBLP:journals/corr/abs-1901-10415}. The work of Lu et al. can be seen as special case of the architecture by He et al. which is why we will cover only the second work. He redefines the forward propagation through equations derived from classic multigrid methods. The Input is cycled down to a coarser resolution and then upscaled again, while in each step the current output is corrected by a residual. The computational overhead is kept small by applying the same weights for down- and upscaling in each cycle. By using residual correction the MGNet be seen as variation of the previously mentioned ResNet.

After having reviewed the most relevant literature we can continue to define the mathematical theory around multigrid methods.

\chapter{Multigrid Methods}
\section{Fundamentals}
\section{Iterative Algorithms}
\section{Convergence Theory}
\chapter{Artificial Neuronal Networks}
To define neuronal networks we will first outline the core concept and workings inside a generic network. Afterwards we are going to define commonly used layers inside convolutional networks and elaborate sophisticated network architectures.
Finally the combination of multigrid methods and convolutional networks is going to be defined.
\section{Base Concepts}
\subsection{Core Definition}
Fundamentally an ANN works as approximation technique that is typically used for complex data relationships that can not be thoroughly defined. The most common example is the previously mentioned classification of images, which will later be used as benchmark task to test network proficiency. \\
A neuronal network can be defined as mapping $N: I \rightarrow T$ where $I$ is the n-dimensional input space and $T$ is the target space. Exact definitions for input and targets depend on the task at hand, e.g. a common example for image classification would be $I \in \mathbb{R}^{C \times H \times W}$ and $T := {\{t_i\}}^{i=1}_{n}$. Here $H \times W$ is the pixel resolution of the image and $C \in \{1,3\}$ is the amount of color channels, i.e. 1 for grayscale and 3 for full color. Each $t_i$ represents one class the input is mapped to. \\
In order to approximate arbitrary data mappings a neuronal network is build out of layers consisting of affine mappings and non-linear activation functions between them. A singular affine mapping can be given by the following definition.  \\
\begin{definition}
For input matrix $\bm{X} \in \mathbb{R}^{H\times W}$, weight vector $w \in \mathbb{R}^{W\times 1}$, bias vector $b \in \mathbb{R}^{H \times 1}$ and activation function $\sigma$ a affine neuronal mapping is defined as
\begin{equation}
\sigma (\bm{X}w+b)
\end{equation}
\end{definition}
For activation function the most common is the rectified linear unit, abbreviated ReLU. \\
\begin{definition}
The function of the form
\begin{equation}
f(x) = \max{0,x}
\end{equation}
is called rectified linear unit (ReLU).
\end{definition}
As this equation partially non-differential and suffers from the vanishing gradient problem we want to introduce a modification derived from the exponential linear unit function ELU
\begin{definition}
The function of the form
\begin{equation}
f_{\alpha}(x) = \begin{cases}
				x, &x\geq0 \\
				\alpha(e^{\frac{x}{\alpha}} - 1), &x < 0
				\end{cases}
\end{equation}
where $\alpha \in (0,\inf)$, is called continuously differentiable exponential linear unit (CELU).
\end{definition}
For $\alpha=1$ it is equal to the ELU function. \\
In general the combination of two or more layers with activation functions is called a block. The base structure of such a network can be seen in Figure XYZ.  \\
In depth a generic layer consists of multiple neurons which represent a mapping defined by multiple weights applied on the input. Each neuron then passes its output to each neuron of the next layer. This is then called a fully-connected layer. With the definition of a mapping above set as neuron we can detail the definition of a layer. \\
\begin{definition}
For layer input $\bm{X} \in \mathbb{R}^{H\times W}$ the $i$-th affine layer $Y^{(i)}$ consisting of $M_i$ neuronal mappings with weight matrix $\bm{W_i} \in \mathbb{R}^{W\times M_i}, M \in  \bm{N}$, bias matrix $\bm{b_i} \in \mathbb{R}^{H\times M_i}$ and activation function $\sigma$ is defined as
\begin{equation}
Y^{(i)}(\bm{X}) := \sigma(\bm{X}\bm{W_i}+\bm{b_i}) \\
\end{equation}
\end{definition}
With its components in place we can now define our network as sequence of layers applied to the input: \\
\begin{definition}
A artificial neuronal network with layers $Y_1,\ldots,Y_N$ and input $\bm{X}$ is defined as
\begin{equation}
{ANN}_{Y_1,\ldots,Y_N}(\bm{X}) := (Y_N \circ Y_{N-1} \circ \ldots \circ Y_{2} \circ Y_{1})(\bm{X})
\end{equation}
\end{definition}
As abbreviation a network is often defined through its forward propagation equation \\
\begin{equation}
\bm{Y}^{(j)}:=\sigma(\bm{Y}^{(j-1)}\bm{W}_j+\bm{b}_j), \ j=1,\ldots,N
\end{equation}
where $\bm{Y}^{0}$ is the initial input $\bm{X}$.\\
The output of the last layer can either already be in the target space, in case of a regression for example, or is fed into a separate function to pass it onto the target space, which is commonly found in classifications. One such function would be the Softmax-function which is defined as such: 
\begin{definition}
A function of the form
\begin{equation}
f(x) = (f(x)_i, \ldots, f(x)_n)^T = \frac{1}{\sum_{j=1}^{n} e^{x_j}} * e^{(\bm{X})}
\end{equation}
where $\sum_{i=1}^{n} f(x)_i = 1, f(x)_i > 0$ is called Softmax.
\end{definition}
where $n$ is the output dimension and simultaneously the number of classes. The resulting vector entries are between \\
As this network is still only general, we have to fit it onto our task with the available data. To achieve this the network is applied on a already correctly labeled subset of the given data, called the training set. The labels can either be class labels for classifications or correct values for regressions. In each iteration we calculate a metric which indicates if the defined network approximates the task at hand properly and is used to correct the weights of the network until it produces a sensible result.
To create such a metric the output of the network and the known targets are put into a so called loss function. As initial definition for arbitrary loss functions we can set the following requirements
\begin{align}
&1. \ S(ANN_{\ell_1,\ldots,\ell_N}(\bm{X}),\bm{T}) = \frac{1}{s} \sum_{i=1}^{s} S(ANN_{\ell_1,\ldots,\ell_N}(x_i),t_i) \\
&2. \  S(ANN_{\ell_1,\ldots,\ell_N}(\bm{X}),T) = S(\bm{Y}^{(N)},\bm{T})
\end{align}
Meaning: \\ 1. The total error equals the arithmetic mean of individual errors \\ 2. The loss can be written as function of the final output \\
For a regression this could be a mean squared error metric or for classifications a cross.entropy loss. The definitions for these are as follows: \\
\begin{definition}
For $y \in \mathbb{R}^n$ and $t \in \mathbb{R}^n$ the function
\begin{equation}
S(\bm{Y}^{(N)},\bm{T}) = \frac{1}{2s}\sum_{i=1}^{s} \| y^{(N)}_{i} - t_i \|^2
\end{equation}
is called mean square error, abbreviated MSE.
\end{definition}
\begin{definition}
For $y \in \mathbb{R}^{n \times k}$ as output of a Softmax function and $t \in \mathbb{N}^k$ as the respective classes the output is mapped to, the function
\begin{equation}
CE(y,t) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k} [\bm{1}_{t_j}(y_i)*\ln(y_{i,j})+(1-\bm{1}_{t_j}(y_{i})))ln(1-y_{i,j})
\end{equation}
where $\bm{1}_{t_j}(y_{i}) = \begin{cases}
							&1, \textit{if} \ y_i \ \textit{belongs to class} \ t_j \\
							&0, \textit{otherwise}
							\end{cases}$
is called cross-entropy loss.
\end{definition}

With the definition of a loss function we can formulate a minimization problem for the approximation task.	
\begin{theorem} \label{minimizationProblem}
Let $\bm{X}=(x_1,\ldots,x_s)^T \in \mathbb{R}^{s \times n}$ and $ANN_{y_1,\ldots,y_N}$ given. Let $S$ be the loss function. With the previous definitions we can state the minimization problem
\begin{equation}
\min_{\bm{\theta}} S(\bm{Y}^{(N)}(\bm{\theta})) + \alpha R(\bm{\theta})
\end{equation}
\end{theorem}
$\alpha$ and $R(\bm{\theta})$ are regularization parameters used to condition the optimization problem, e.g. $L2-$ or tikhonov regularization.
\begin{definition}
A regularization parameter $\alpha R(\bm{\theta})$ for the optimization problem \ref{minimizationProblem}
is called $L2$ regularization, if
\begin{equation}
\alpha R(\bm{\theta}) = \lambda \parallel \bm{\theta} \parallel_2^2 , \ \lambda \in \mathbb{R}
\end{equation}
\end{definition}
To now shift our weights in a way that produces a lower loss metric we calculate the gradient of the loss respective to all applied weights and correct them through a optimization function. In the simplest form this can be done through batch gradient descent function which is defined here \\
\begin{definition}
Das Optimierungsverfahren für das Minimierungsproblem \ref{minimizationProblem} mit der Iterationsvorschrift 
\begin{equation}
\bm{\theta}_{j+1} = \bm{\theta}_j - \frac{\eta}{k} \sum_{i=1}^{k} \nabla_{\bm{\theta}} S(\bm{Y}^{(N)}_{i}(\bm{\theta}_j))
\end{equation}
heißt 
\begin{itemize}
\item batch Gradientenverfahren, falls $k=s$
\item stochastisches Gradientenverfahren, falls $k=1$
\item mini-batch Gradientenverfahren, falls $1<k<s$
\end{itemize}
\end{definition}
A batch is the subset of the labeled data that is processed at once. \\
The process of correcting the weights is called backpropagation and is repeated multiple times on the whole available data until the resulting loss is deemed low enough or until no further reduction can be achieved. With our previous definition we can define the backpropagation algorithm as such: \\
\begin{algorithm}[H] \label{backprop}
\caption{Backpropagation}
 \KwData{Output $\bm{Y}^{(N)}$ und Ableitungen $\sigma'(\bm{Z}^{(j)})$}
  $\delta^{(N)} = \nabla S \circ \sigma'(\bm{Z}^{(N)})$ \\
 \For{$j=N-1,\ldots,1$}{
  $\delta^{(j)} = (\delta^{(k+1)}\bm{W}^{(k+1)}) \circ \sigma'(Z^{(k)})$ \\
 }
 \For{$j=1,\ldots,N$}{
 \For{$k=1,\ldots,m_j$}{
  	$\frac{\partial S}{\partial w^{(j)}_{k,i}}= \delta^{(j)}_ky^{(j-1)}_{i}$ \\
   $\frac{\partial S}{\partial b^{(j)}_{k,i}}= \delta^{(j)}_k$\\
 } 
 }
 \KwResult{Gradienten $\frac{\partial S}{\partial w^{(j)}_{k,i}},\frac{\partial S}{\partial b^{(j)}_{k,i}}$}
\end{algorithm}
The final performance of the network is tested on a not previously used set of the data, called validation set. \\
\subsection{Convolutional Networks}
With the now defined networks we can theoretically process all types of input. But it shows that for more complex input like images we need a more sophisticated approach than affine mappings. For this task the convolution layers established themselves as main building block and gave the commonly known convolutional neuronal network its name. \\
Convolution layers take in a three dimensional input consisting of $C_0$ $H \times W$ matrices and are defined by $C_1$ pairs of $C_0$ $k \times l$-dimensional kernels, whose entries are trained through backpropagation. $C_0$ and $C_1$ are often called the number of in- and output-channels. For each matrix in a channel of the input there exists at least one kernel that is applied on it that produces a new $n \times m$ matrix, called feature map. The entries of the feature map are calculated through the inner product of the kernel with an excerpt of the input matrix of the same size as the kernel. The inner product of two matrices is defined as follows: \\

The kernel is shifted across the input matrix and produces a feature map entry in each step. The number of elements the kernel is moved along the matrix in each step is called the step-size. Increasing the size of the input matrix through a border of constant entries is called padding. A visualization of this process can be found in Figure XYZ. \\
 
The resulting maps of the $C_0$ kernels are then summed up element wise into a $n \times m$ output matrix. This is repeated until there are $C_1$ channels created. Thus the final output is again three dimensional of the form $C_1 \times n \times m$ and the whole process can be defined as such: \\
\begin{definition}
Die Funktion $P_k$ definiert durch
\begin{align*}
&P_k : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}^{(n+2k) \times (m+2k)} \\
&P_k(A) = A^* 
\end{align*}
heißt padding-Funktion,
wobei $A^*_{[1:2k,:]} = A^*_{[n+1:n+2k,:]} = A^*_{[:,0:2k]} = A^*_{[:,m+1:m+2k]} = 0$.
\end{definition}
Eine padding-Funktion $P_k$ fügt also an eine bestehende Matrix A an allen Seiten genau $k$ Nullzeilen an
\begin{definition}
Die Funktionen $S^{h}, S^{v}$ definiert durch
\begin{align*}
&S^{i}_{s,t} : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}^{n \times m}, i \in \lbrace h,v \rbrace \\
&S^{h}_{s,t}(A_{[i:j,p:q]}) = A_{[i+t*s:j+t*s,p:q]} \\
&S^{v}_{s,t}(A_{[i:j,p:q]}) = A_{[i:j,p+t*s:q+t*s]} 
\end{align*}
heißen stride-Funktionen.
\end{definition}
\begin{definition} \label{convlayer}
Sei der Input-Vektor $\bm{X} \in \mathbb{R}^{h \times w \times d}$ gegeben, $h,w,d \in \mathbb{N}$.
Seien $F^i = (\bm{W}_{1,i},\ldots,\bm{W}_{d,i}) \in \mathbb{R}^{n \times n \times d}$ Filter, $i=1,\ldots ,k$, mit Bias $b^i \in \mathbb{R}^{n \times n  \times n}$. Weiterhin sei $s \in \mathbb{N}$ die Schrittweite zu den entsprechenden stride-Funktionen $S^{h}, S^{v}$ und $P_t$ eine padding-Funktion mit Parameter $t \in \mathbb{N}$, $\sigma$ eine Aktivierungsfunktion die elementweise angewandt wird.
Eine Faltungs-Ebene $\ell^{conv}_{k,F_i,s,\sigma}$ ist definiert als Funktion
\begin{align}
&\ell^{conv}_{(n,k),s,t,\sigma} : \mathbb{R}^{h \times w \times d} \rightarrow \mathbb{R}^{(\frac{(h-n+2*t)}{S}+1) \times (\frac{(w-n+2*t)}{S}+1) \times k} =: \mathbb{R}^{h^* \times w^* \times k} \\
&\ell^{conv}_{(n,k),s,t,\sigma}(\bm{X})_{[p,q,i]} = \sigma(\sum_{j=1}^{d}(\sum_{1 \leq a \leq n, 1 \leq b \leq n} (S^{h}_{s,p}(S^{v}_{s,q}(P_t(\bm{X})_{[1:n,1:n, \ j]})) \circ F^{i}_{j})_{[a,b]})
\end{align}
\end{definition}
Convolutional layers are capable of processing more dimensional input better than affine layers and can lead to drastically improved results. \\

Next we need to introduce so called pooling layers. 
Instead of calculating a inner product of a kernel with the input, a pooling layer only has a kernel size parameter to define the window on which its function is applied on in each step. Contrary to convolutions they always keep the number of channels that is passed on as output constant and only change the dimensions of the channel matrices. Similar to convolutions they are further defined through a step- and padding-size parameter, but their applied functions are fixed and do not require training. \\
An example would be the so called max-pooling, which takes the maximum of all numbers inside the kernel window and returns it as an output entry. A short overview of pooling functions is given here: \\
\begin{definition}
Sei der Input-Vektor $\bm{X} \in \mathbb{R}^{h \times w \times d}$ gegeben, $h,w,d \in \mathbb{N}$. Sei weiter $\phi : R^{n \times n \times d}$ die pooling-Funktion der Ebene und $s \in \mathbb{N}$ die Schrittweite der stride-Funktionen $S^{h}, S^{v}$.
Dann ist die daraus konstruierte pooling-Ebene $\ell^{pool}_{n,s}$ wie folgt definiert.
\begin{align*}
\ell^{pool}_{n,s} : \mathbb{R}^{h \times w \times d} \rightarrow \mathbb{R}^{(\frac{(h-n)}{S}+1) \times (\frac{(w-n)}{S}+1) \times d} \\
\ell^{pool}_{n,s}(\bm{X})_{[p,q,i]} := \phi(S^{h}_{s,p}(S^{v}_{s,q}(\bm{X}_{[1:n,1:n,i]})))
\end{align*}
\end{definition}
In der Praxis werden 3 Arten von pooling-Ebenen Verwendet: max-pooling \cite{MaxPool}, average-pooling und $L_2$-pooling \cite{Goodfellow-et-al-2016}. Der Name steht dabei für die Form von $\phi$. Ein Beispiel für eine solche Ebene ist in Abbildung dargestellt.
\begin{definition}
Sei $A = (a_{i,j}) \in \mathbb{R}^{n \times m}$. 
Wir nennen eine pooling-Ebene $\ell^{pool}_{n,s}$
\begin{itemize}
\item max-pooling-Ebene (kurz max-pool), falls
\begin{equation}
\phi(A) = \max_{i,j} a_{i,j}
\end{equation}
\item avgerage-pooling-Ebene (kurz avg-pool), falls
\begin{equation}
\phi(A) = \frac{1}{n \times m} \sum_{i,j} a_{i,j}
\end{equation}
\item $L_2$-pooling-Ebene (kurz l2-pool), falls
\begin{equation}
\phi(A) = \sqrt{\sum_{i,j} a_{i,j}^2}
\end{equation}
\end{itemize}
\end{definition}
Pooling layers are often used to reduce dimensions without introducing additional parameters into the network.
  
\section{Sophisticated Network Architectures}
ResNet
RoR
DenseNet
FractalNet
\chapter{Application of Multigrid Methods in CNNs}
MGNet
\chapter{Implementation and empirical Analysis}
\section{Chosen Network Architectures}
\section{Realization in Python}
\section{Analysis}
\chapter{Conclusion}

\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{references} \label{bibtex}
\end{document}